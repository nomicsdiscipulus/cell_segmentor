{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fb55f1-f7c3-4bd4-b05c-a6696087af7b",
   "metadata": {},
   "source": [
    "# Feature Extraction using DinoBloom on BCCD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd000d-c482-4c61-95a3-736ee3c0c731",
   "metadata": {},
   "source": [
    "Test DinoBloom feature extraction on BCCD dataset.\n",
    "\n",
    "This script tests the DinoBloom model by:\n",
    "1. Loading the pre-trained DinoBloom-S model (smallest, 22M params)\n",
    "2. Extracting patch features from BCCD test images\n",
    "3. Visualizing features using PCA\n",
    "\n",
    "Designed to run on CPU for local testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "285ac88e-7157-4f12-8c49-fd5fef531157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713dae4b-75a9-42f3-b566-6739bf45f398",
   "metadata": {},
   "source": [
    "## PATH Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58712efb-3294-47bf-b559-471bbafc9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRIPT_DIR = Path(__file__).parent                    # 03_inference/\n",
    "SCRIPT_DIR = Path.cwd()\n",
    "#SCRIPT_DIR = Path(\"D:/A_Jobs/Merck/Fd-To-Sg/Codes/unitTest/03_inference\") # if one does not run in the file's folder.\n",
    "UNITTEST_DIR = SCRIPT_DIR.parent                      # unitTest/\n",
    "CODES_DIR = UNITTEST_DIR.parent                       # Codes/\n",
    "PROJECT_ROOT = CODES_DIR.parent                       # Fd-To-Sg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ed677b9-8510-409d-bd10-7983a599559f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('D:/A_Jobs/Merck/Fd-To-Sg/Codes/unitTest/03_inference'),\n",
       " WindowsPath('D:/A_Jobs/Merck/Fd-To-Sg/Codes/unitTest'),\n",
       " WindowsPath('D:/A_Jobs/Merck/Fd-To-Sg/Codes'),\n",
       " WindowsPath('D:/A_Jobs/Merck/Fd-To-Sg'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCRIPT_DIR, UNITTEST_DIR, CODES_DIR, PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f764157-e7ab-4634-86cb-85470eec6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, str(CODES_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d70f310c-b44a-49cb-8a67-e75344c486b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "DATA_ROOT = PROJECT_ROOT / \"Data\" / \"BCCD\"\n",
    "TEST_IMAGES_DIR = DATA_ROOT / \"test\" / \"original\"\n",
    "TEST_MASKS_DIR = DATA_ROOT / \"test\" / \"mask\"\n",
    "OUTPUT_DIR = SCRIPT_DIR / \"outputs\" / \"01_feature_extraction\" / \"large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f99d7e78-07ff-4254-bd70-49adb9964da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('D:/A_Jobs/Merck/Fd-To-Sg/Data/BCCD'),\n",
       " WindowsPath('D:/A_Jobs/Merck/Fd-To-Sg/Data/BCCD/test/original'),\n",
       " WindowsPath('D:/A_Jobs/Merck/Fd-To-Sg/Data/BCCD/test/mask'),\n",
       " WindowsPath('D:/A_Jobs/Merck/Fd-To-Sg/Codes/unitTest/outputs'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT, TEST_IMAGES_DIR, TEST_MASKS_DIR, OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116cd46-a4d8-4f55-a74a-86b97f71c17b",
   "metadata": {},
   "source": [
    "## LIBRARY Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6c3e302-4335-49d2-b0bc-6c03158f1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "\n",
    "# Import from our backbone wrapper\n",
    "from backbone import load_dinobloom, extract_features, IMAGENET_MEAN, IMAGENET_STD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50a6db-6a14-433f-9fa7-0fd2beaa0903",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb27106f-ef53-4763-bf50-254dcf0a67df",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = \"large\"  # \"small\", \"base\", or \"large\"\n",
    "IMG_SIZE = 224  # Must be divisible by 14. Options: 224, 518, 728, etc.\n",
    "PATCH_SIZE = 14\n",
    "PATCH_NUM = IMG_SIZE // PATCH_SIZE  # 16x16 patches for 224, 37x37 for 518\n",
    "\n",
    "# Output options\n",
    "SAVE_RESIZED_IMAGES = True  # Save individual resized images\n",
    "SAVE_COMPARISON = True      # Save side-by-side comparison (original vs resized vs features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d75523b-f07e-41f8-992b-d5f253b5bd27",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dca4a62b-4f4b-4ca0-9b19-5e8b1f0ba78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(img: Image.Image, target_size: int, method: str = \"resize\") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Resize image to target size using specified method.\n",
    "\n",
    "    Args:\n",
    "        img: PIL Image\n",
    "        target_size: Target size (square output)\n",
    "        method:\n",
    "            - \"resize\": Direct resize (may distort aspect ratio)\n",
    "            - \"center_crop\": Resize shortest side, then center crop\n",
    "            - \"pad\": Resize longest side, then pad to square\n",
    "\n",
    "    Returns:\n",
    "        Resized PIL Image (target_size × target_size)\n",
    "    \"\"\"\n",
    "    if method == \"resize\":\n",
    "        return img.resize((target_size, target_size), Image.BILINEAR)\n",
    "\n",
    "    elif method == \"center_crop\":\n",
    "        # Resize so shortest side = target_size, then center crop\n",
    "        w, h = img.size\n",
    "        scale = target_size / min(w, h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        img = img.resize((new_w, new_h), Image.BILINEAR)\n",
    "\n",
    "        # Center crop\n",
    "        left = (new_w - target_size) // 2\n",
    "        top = (new_h - target_size) // 2\n",
    "        return img.crop((left, top, left + target_size, top + target_size))\n",
    "\n",
    "    elif method == \"pad\":\n",
    "        # Resize so longest side = target_size, then pad\n",
    "        w, h = img.size\n",
    "        scale = target_size / max(w, h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        img = img.resize((new_w, new_h), Image.BILINEAR)\n",
    "\n",
    "        # Pad to square (black padding)\n",
    "        padded = Image.new('RGB', (target_size, target_size), (0, 0, 0))\n",
    "        left = (target_size - new_w) // 2\n",
    "        top = (target_size - new_h) // 2\n",
    "        padded.paste(img, (left, top))\n",
    "        return padded\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown resize method: {method}\")\n",
    "\n",
    "\n",
    "def get_image_info(img_path: Path) -> dict:\n",
    "    \"\"\"Get image metadata.\"\"\"\n",
    "    img = Image.open(img_path)\n",
    "    return {\n",
    "        \"path\": img_path,\n",
    "        \"size\": img.size,\n",
    "        \"aspect_ratio\": img.size[0] / img.size[1],\n",
    "        \"format\": img.format\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Data Loading\n",
    "# =============================================================================\n",
    "\n",
    "def get_image_paths(images_dir: Path, num_samples: int = 4) -> list:\n",
    "    \"\"\"Get random sample of image paths from directory.\"\"\"\n",
    "    all_images = list(images_dir.glob(\"*.png\")) + list(images_dir.glob(\"*.jpg\"))\n",
    "    if len(all_images) < num_samples:\n",
    "        return all_images\n",
    "    return random.sample(all_images, num_samples)\n",
    "\n",
    "\n",
    "def load_and_preprocess_images(\n",
    "    image_paths: list,\n",
    "    transform,\n",
    "    target_size: int = IMG_SIZE,\n",
    "    resize_method: str = \"resize\"\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Load images and apply preprocessing.\n",
    "\n",
    "    Args:\n",
    "        image_paths: List of image file paths\n",
    "        transform: Torchvision transform for normalization\n",
    "        target_size: Target image size (must be divisible by 14)\n",
    "        resize_method: \"resize\", \"center_crop\", or \"pad\"\n",
    "\n",
    "    Returns:\n",
    "        tuple: (tensor of transformed images, list of original PIL images, list of resized PIL images)\n",
    "    \"\"\"\n",
    "    images_original = []\n",
    "    images_resized = []\n",
    "    tensors = []\n",
    "\n",
    "    for path in image_paths:\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        images_original.append(img.copy())\n",
    "\n",
    "        img_resized = resize_image(img, target_size, method=resize_method)\n",
    "        images_resized.append(img_resized)\n",
    "        tensors.append(transform(img_resized))\n",
    "\n",
    "    return torch.stack(tensors), images_original, images_resized\n",
    "\n",
    "# =============================================================================\n",
    "# Visualization\n",
    "# =============================================================================\n",
    "\n",
    "def save_resized_images(\n",
    "    image_paths: list,\n",
    "    images_resized: list,\n",
    "    output_dir: Path,\n",
    "    resize_method: str = \"resize\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Save resized images to disk for inspection.\n",
    "\n",
    "    Args:\n",
    "        image_paths: Original image paths (for naming)\n",
    "        images_resized: List of resized PIL images\n",
    "        output_dir: Directory to save images\n",
    "        resize_method: Method used for resizing (for filename)\n",
    "    \"\"\"\n",
    "    resized_dir = output_dir / \"resized_images\"\n",
    "    resized_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for path, img in zip(image_paths, images_resized):\n",
    "        output_name = f\"{path.stem}_{resize_method}_{img.size[0]}x{img.size[1]}.png\"\n",
    "        output_path = resized_dir / output_name\n",
    "        img.save(output_path)\n",
    "        print(f\"  Saved: {output_path.name}\")\n",
    "\n",
    "    print(f\"Resized images saved to: {resized_dir}\")\n",
    "\n",
    "\n",
    "def visualize_resize_comparison(\n",
    "    image_paths: list,\n",
    "    images_original: list,\n",
    "    images_resized: list,\n",
    "    output_path: Path,\n",
    "    resize_method: str = \"resize\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create side-by-side comparison of original vs resized images.\n",
    "\n",
    "    Args:\n",
    "        image_paths: Original image paths (for titles)\n",
    "        images_original: List of original PIL images\n",
    "        images_resized: List of resized PIL images\n",
    "        output_path: Path to save the comparison figure\n",
    "        resize_method: Method used for resizing\n",
    "    \"\"\"\n",
    "    num_images = len(images_original)\n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(10, 5 * num_images))\n",
    "\n",
    "    if num_images == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, (path, orig, resized) in enumerate(zip(image_paths, images_original, images_resized)):\n",
    "        # Original\n",
    "        axes[i, 0].imshow(orig)\n",
    "        axes[i, 0].set_title(f\"Original: {path.name}\\n{orig.size[0]}x{orig.size[1]}\")\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Resized\n",
    "        axes[i, 1].imshow(resized)\n",
    "        axes[i, 1].set_title(f\"Resized ({resize_method})\\n{resized.size[0]}x{resized.size[1]}\")\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Original vs Resized Images (method: {resize_method})\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Saved comparison to {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_pca_features(features, images_for_plotting, output_path: Path, upsample: bool = True):\n",
    "    \"\"\"\n",
    "    Visualize patch features using PCA reduction to RGB.\n",
    "\n",
    "    Args:\n",
    "        features: Tensor of shape (N, num_patches, embed_dim)\n",
    "        images_for_plotting: List of PIL images\n",
    "        output_path: Path to save the visualization\n",
    "        upsample: If True, upsample feature map to match image size for better visualization\n",
    "    \"\"\"\n",
    "    num_images = len(images_for_plotting)\n",
    "    embed_dim = features.shape[-1]\n",
    "    img_size = images_for_plotting[0].size[0]  # Assuming square images\n",
    "    patch_num = int(np.sqrt(features.shape[1]))  # Infer patch grid size\n",
    "\n",
    "    # Reshape features for PCA: (N * H * W, embed_dim)\n",
    "    features_flat = features.reshape(num_images * patch_num * patch_num, embed_dim).cpu().numpy()\n",
    "\n",
    "    # Fit PCA to reduce to 3 components (RGB)\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(features_flat)\n",
    "    pca_features = pca.transform(features_flat)\n",
    "\n",
    "    # Normalize each component to [0, 1]\n",
    "    for i in range(3):\n",
    "        pca_features[:, i] = (pca_features[:, i] - pca_features[:, i].min()) / \\\n",
    "                             (pca_features[:, i].max() - pca_features[:, i].min() + 1e-8)\n",
    "\n",
    "    # Reshape back to images\n",
    "    pca_features_rgb = pca_features.reshape(num_images, patch_num, patch_num, 3)\n",
    "\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, num_images, figsize=(4 * num_images, 8))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Resized input image\n",
    "        axes[0, i].imshow(images_for_plotting[i])\n",
    "        axes[0, i].set_title(f\"Input ({img_size}x{img_size})\")\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        # PCA features (optionally upsampled)\n",
    "        if upsample:\n",
    "            # Upsample feature map to match input image size\n",
    "            pca_img = Image.fromarray((pca_features_rgb[i] * 255).astype(np.uint8))\n",
    "            pca_img_upsampled = pca_img.resize((img_size, img_size), Image.NEAREST)\n",
    "            axes[1, i].imshow(pca_img_upsampled)\n",
    "            axes[1, i].set_title(f\"PCA Features\\n({patch_num}x{patch_num} → {img_size}x{img_size})\")\n",
    "        else:\n",
    "            axes[1, i].imshow(pca_features_rgb[i])\n",
    "            axes[1, i].set_title(f\"PCA Features ({patch_num}x{patch_num})\")\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "    plt.suptitle(f\"DinoBloom Feature Extraction (patch_size={PATCH_SIZE}, grid={patch_num}x{patch_num})\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Saved visualization to {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "    return pca_features_rgb, patch_num\n",
    "\n",
    "# =============================================================================\n",
    "# Main\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DinoBloom Feature Extraction Test on BCCD Dataset\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Print configuration\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Model size:    {MODEL_SIZE}\")\n",
    "    print(f\"  Image size:    {IMG_SIZE}x{IMG_SIZE}\")\n",
    "    print(f\"  Patch size:    {PATCH_SIZE}x{PATCH_SIZE}\")\n",
    "    print(f\"  Patch grid:    {PATCH_NUM}x{PATCH_NUM} = {PATCH_NUM**2} patches\")\n",
    "\n",
    "    print(f\"\\nPath Configuration:\")\n",
    "    print(f\"  Project root: {PROJECT_ROOT}\")\n",
    "    print(f\"  Data root:    {DATA_ROOT}\")\n",
    "    print(f\"  Output:       {OUTPUT_DIR}\")\n",
    "\n",
    "    if not TEST_IMAGES_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Test images directory not found: {TEST_IMAGES_DIR}\")\n",
    "\n",
    "    # Create output directory\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load model using backbone wrapper\n",
    "    print(f\"\\nLoading DinoBloom model (size={MODEL_SIZE})...\")\n",
    "    model = load_dinobloom(size=MODEL_SIZE, device=\"cpu\")\n",
    "\n",
    "    # Setup transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ])\n",
    "\n",
    "    # Load test images\n",
    "    resize_method = \"resize\"  # Options: \"resize\", \"center_crop\", \"pad\"\n",
    "    print(f\"\\nLoading test images (resize_method={resize_method})...\")\n",
    "    image_paths = get_image_paths(TEST_IMAGES_DIR, num_samples=4)\n",
    "    print(f\"Selected {len(image_paths)} images:\")\n",
    "    for p in image_paths:\n",
    "        info = get_image_info(p)\n",
    "        print(f\"  - {p.name} ({info['size'][0]}x{info['size'][1]})\")\n",
    "\n",
    "    imgs_tensor, images_original, images_resized = load_and_preprocess_images(\n",
    "        image_paths, transform, target_size=IMG_SIZE, resize_method=resize_method\n",
    "    )\n",
    "    print(f\"Input tensor shape: {imgs_tensor.shape}\")\n",
    "\n",
    "    # Save resized images if enabled\n",
    "    if SAVE_RESIZED_IMAGES:\n",
    "        print(\"\\nSaving resized images...\")\n",
    "        save_resized_images(image_paths, images_resized, OUTPUT_DIR, resize_method)\n",
    "\n",
    "    # Save comparison visualization if enabled\n",
    "    if SAVE_COMPARISON:\n",
    "        print(\"\\nGenerating resize comparison...\")\n",
    "        comparison_path = OUTPUT_DIR / \"resize_comparison.png\"\n",
    "        visualize_resize_comparison(\n",
    "            image_paths, images_original, images_resized, comparison_path, resize_method\n",
    "        )\n",
    "\n",
    "    # Extract features using backbone wrapper\n",
    "    print(\"\\nExtracting features...\")\n",
    "    features = extract_features(model, imgs_tensor)\n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "    print(f\"  - {features.shape[0]} images\")\n",
    "    print(f\"  - {features.shape[1]} patches per image ({PATCH_NUM}x{PATCH_NUM})\")\n",
    "    print(f\"  - {features.shape[2]} embedding dimension\")\n",
    "\n",
    "    # Visualize PCA features\n",
    "    print(\"\\nGenerating PCA visualization...\")\n",
    "    output_path = OUTPUT_DIR / \"dinobloom_bccd_features.png\"\n",
    "    visualize_pca_features(features, images_resized, output_path, upsample=True)\n",
    "\n",
    "    # Also save non-upsampled version for comparison\n",
    "    output_path_raw = OUTPUT_DIR / \"dinobloom_bccd_features_raw.png\"\n",
    "    visualize_pca_features(features, images_resized, output_path_raw, upsample=False)\n",
    "\n",
    "    print(\"\\nTest completed successfully!\")\n",
    "    print(f\"\\nOutputs saved to: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3964f62d-2c5a-46ad-8753-efdc47418cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chunr\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7a48615-72e2-474d-90e3-de5bdbfdc96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DinoBloom Feature Extraction Test on BCCD Dataset\n",
      "============================================================\n",
      "\n",
      "Configuration:\n",
      "  Model size:    large\n",
      "  Image size:    224x224\n",
      "  Patch size:    14x14\n",
      "  Patch grid:    16x16 = 256 patches\n",
      "\n",
      "Path Configuration:\n",
      "  Project root: D:\\A_Jobs\\Merck\\Fd-To-Sg\n",
      "  Data root:    D:\\A_Jobs\\Merck\\Fd-To-Sg\\Data\\BCCD\n",
      "  Output:       D:\\A_Jobs\\Merck\\Fd-To-Sg\\Codes\\unitTest\\03_inference\\outputs\\01_feature_extraction\\large\n",
      "\n",
      "Loading DinoBloom model (size=large)...\n",
      "Loading DinoBloom-LARGE (304M params)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\chunr/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth\" to C:\\Users\\chunr/.cache\\torch\\hub\\checkpoints\\dinov2_vitl14_pretrain.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 1.13G/1.13G [00:53<00:00, 22.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Embed dim: 1024\n",
      "\n",
      "Loading test images (resize_method=resize)...\n",
      "Selected 4 images:\n",
      "  - fbc1f360-4382-48bb-a32e-3f5f2f9dbe57.png (1600x1200)\n",
      "  - eb6e8684-b84e-483d-b835-f59286d2516b.png (1600x1200)\n",
      "  - f5c395f1-f94c-440d-8221-cc656e52c0b1.png (1600x1200)\n",
      "  - fe851c88-692d-4199-87e0-d19d9c4eb591.png (1600x1200)\n",
      "Input tensor shape: torch.Size([4, 3, 224, 224])\n",
      "\n",
      "Saving resized images...\n",
      "  Saved: fbc1f360-4382-48bb-a32e-3f5f2f9dbe57_resize_224x224.png\n",
      "  Saved: eb6e8684-b84e-483d-b835-f59286d2516b_resize_224x224.png\n",
      "  Saved: f5c395f1-f94c-440d-8221-cc656e52c0b1_resize_224x224.png\n",
      "  Saved: fe851c88-692d-4199-87e0-d19d9c4eb591_resize_224x224.png\n",
      "Resized images saved to: D:\\A_Jobs\\Merck\\Fd-To-Sg\\Codes\\unitTest\\03_inference\\outputs\\01_feature_extraction\\large\\resized_images\n",
      "\n",
      "Generating resize comparison...\n",
      "Saved comparison to D:\\A_Jobs\\Merck\\Fd-To-Sg\\Codes\\unitTest\\03_inference\\outputs\\01_feature_extraction\\large\\resize_comparison.png\n",
      "\n",
      "Extracting features...\n",
      "Features shape: torch.Size([4, 256, 1024])\n",
      "  - 4 images\n",
      "  - 256 patches per image (16x16)\n",
      "  - 1024 embedding dimension\n",
      "\n",
      "Generating PCA visualization...\n",
      "Saved visualization to D:\\A_Jobs\\Merck\\Fd-To-Sg\\Codes\\unitTest\\03_inference\\outputs\\01_feature_extraction\\large\\dinobloom_bccd_features.png\n",
      "Saved visualization to D:\\A_Jobs\\Merck\\Fd-To-Sg\\Codes\\unitTest\\03_inference\\outputs\\01_feature_extraction\\large\\dinobloom_bccd_features_raw.png\n",
      "\n",
      "Test completed successfully!\n",
      "\n",
      "Outputs saved to: D:\\A_Jobs\\Merck\\Fd-To-Sg\\Codes\\unitTest\\03_inference\\outputs\\01_feature_extraction\\large\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
